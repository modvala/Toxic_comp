{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLDR: Inspired by the Jeremy's lstm, i tried using the convolutional layer on the top of that. Collectivley these type of codes are Long-term Recurrent Convolutional Networks or CNN-LSTM networks Here are few links explaining its effectiveness https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/\n",
    "\n",
    "Adding the CNN on top of LSTM helps in the sense that CNN combined with pooling layers can bring out important temporal features devoid of any noise which the LSTM can use more effectively. In the end, bidirectional LSTM will help in classifying the data.\n",
    "\n",
    "Let's go to the code directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanupDoc(s):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stopset.add('wikipedia')\n",
    "    tokens =sequence=text_to_word_sequence(s, \n",
    "                                        filters=\"\\\"!'#$%&()*+,-˚˙./:;‘“<=·>?@[]^_`{|}~\\t\\n\",\n",
    "                                        lower=True,\n",
    "                                        split=\" \")\n",
    "    cleanup = \" \".join(filter(lambda word: word not in stopset, tokens))\n",
    "    return cleanup\n",
    "\n",
    "test.comment_text=test.comment_text.apply(cleanupDoc)\n",
    "train.comment_text=train.comment_text.apply(cleanupDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.comment_text)\n",
    "sequences = tokenizer.texts_to_sequences(train.comment_text)\n",
    "test_sequence=tokenizer.texts_to_sequences(test.comment_text)\n",
    "data = pad_sequences(sequences, maxlen=150)\n",
    "t_data = pad_sequences(test_sequence, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the pretrained glove vectors\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('../input/glove6b300dtxt/glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_length=300\n",
    "length=150\n",
    "num_classes=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "nb_words = min(200000, len(word_index))\n",
    "notfound=[]\n",
    "embedding_matrix = np.zeros((nb_words, vector_length))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        notfound.append(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras stuff now\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "from keras.layers import LSTM, Convolution1D,Convolution2D, Flatten, Dropout, Dense, Input, Conv1D, GRU, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Merge, merge, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "#Using Pretrained Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getjmodel():\n",
    "    inp = Input(shape=(length,))\n",
    "    x = Embedding(nb_words, vector_length, weights=[embedding_matrix])(inp)\n",
    "    x = Conv1D(256, 3, activation='relu')(x)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=train[['toxic','severe_toxic', 'obscene','threat','insult', 'identity_hate' ]]\n",
    "data_train, data_test, y_train, y_test, comm_train, comm_trst =train_test_split(data, np.array(labels),train.comment_text, test_size=0.20, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "galaxyModel=getjmodel()\n",
    "galaxyModel.fit(data_train, y_train, 1024, epochs=3, validation_data=(data_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds=galaxyModel.predict(t_data)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission[list_classes] = preds\n",
    "sample_submission.to_csv('mysubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
