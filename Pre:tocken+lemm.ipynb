{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import textacy\n",
    "#import en_core_web_md\n",
    "import sematch\n",
    "import gensim\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "    \n",
    "from clean_utils import *\n",
    "from parser_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from twit_prepocess import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cocksucker <allcaps> before <allcaps> you <allcaps> piss <allcaps> around <allcaps> on <allcaps> my <allcaps> work <allcaps>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"I TEST alllll kinds of #hashtags and #HASHTAGS, @mentions and 3000 (http://t.co/dkfjkdf). w/ <3 :) haha!!!!!\"\n",
    "tokenize(data.comment_text[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.comment_text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_stat(word):\n",
    "    dlst = len(word)\n",
    "    n_punc = len([i for i in word if i in string.punctuation])\n",
    "    n_upper = len([i for i in word if i.isupper()])\n",
    "    return np.array([dlst, n_punc, n_upper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment=comment.lower()\n",
    "comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "#comment = myDate.sub(' xxDATExx ', comment)\n",
    "comment=re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\",comment)\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]   \n",
    "    clean_sent=\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lemmat_quora(df, shapes):\n",
    "    t = time.time()\n",
    "    print('Cleaning based on forums functions.', df.shape)\n",
    "    df = df.apply(lambda x :clean(x))\n",
    "    print('Cleaning using textacy.', df.shape)\n",
    "    df = df.apply(lambda x: textacy.preprocess.preprocess_text(x, fix_unicode = True,\n",
    "                                                           no_urls=True, no_emails=True, \n",
    "                                                               no_phone_numbers=True, no_numbers=True,\n",
    "                                                                            lowercase = True,\n",
    "                                                                            no_contractions = True,\n",
    "                                                                            transliterate = True))\n",
    "    print('Lemmatizing text.', df.shape)\n",
    "    SYMBOLS = set(' '.join(string.punctuation).split(' ') + ['...', '“', '”', '\\'ve'])\n",
    "    q1 = []\n",
    "    c = 0\n",
    "    for doc in tqdm(nlp.pipe(df, n_threads=8, batch_size=10000)):\n",
    "        c+=1\n",
    "        word_list = ([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "        q1.append(' '.join(i for i in word_list))\n",
    "    q1 = pd.DataFrame(q1, columns=['comment_text'])\n",
    "    df_train_lemm = q1.iloc[:shapes[0]]\n",
    "    df_test_lemm = q1.iloc[shapes[0]:]\n",
    "    print('Correcting words. Using most probable substitutes.', q1.shape)\n",
    "    q1.comment_text = q1.comment_text.apply(lambda x: (' '.join([correction(i) for i in word_tokenize(x)])))\n",
    "    df_train_correct = q1.iloc[:shapes[0]]\n",
    "    df_test_correct = q1.iloc[shapes[0]:]\n",
    "    print('Text cleaning done, time it took:', time.time() - t, q1.shape)\n",
    "    return df_train_lemm, df_test_lemm, df_train_correct, df_test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NROWS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv('data/train.csv', sep=',', index_col='id', nrows=NROWS)\n",
    "shapes = data.shape\n",
    "data['train'] = 1\n",
    "data = pd.concat([data, pd.read_csv('data/test.csv', sep=',', index_col='id', nrows=NROWS)], axis=0)\n",
    "data.train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('vocs/crawl-300d-2M.vec')\n",
    "\n",
    "words = model.index2word\n",
    "\n",
    "%%time\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocs/words_for_check_spell.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(WORDS, 'vocs/words_for_check_spell.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = joblib.load('vocs/words_for_check_spell.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning based on forums functions. (312735,)\n",
      "Cleaning using textacy. (312735,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing text. (312735,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312735it [48:25, 107.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting words. Using most probable substitutes. (312735, 1)\n",
      "Text cleaning done, time it took: 3450.4344255924225 (312735, 1)\n"
     ]
    }
   ],
   "source": [
    "df_train_lemm, df_test_lemm, df_train_corr, df_test_corr = clean_lemmat_quora(data.comment_text, shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 1), (153164, 1), (159571, 1), (153164, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_corr.shape, df_test_corr.shape, df_train_lemm.shape, df_test_lemm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_corr.to_csv('pre_data/df_train_spacycorr.csv', index = True, sep=';', index_label='index')\n",
    "df_test_corr.to_csv('pre_data/df_test_spacycorr.csv', index = True, sep=';', index_label='index')\n",
    "df_train_lemm.to_csv('pre_data/df_train_spacylemm.csv', index = True, sep=';', index_label='index')\n",
    "df_test_lemm.to_csv('pre_data/df_test_spacylemm.csv', index = True, sep=';', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanationwhy edit make username hardcore met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d'aww match background colour i be seemingly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man i be really try edit war -PRON- be guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>morei can not make real suggestion improvement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir hero chance remember page that be</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                       comment_text\n",
       "0      0  explanationwhy edit make username hardcore met...\n",
       "1      1  d'aww match background colour i be seemingly s...\n",
       "2      2  hey man i be really try edit war -PRON- be guy...\n",
       "3      3  morei can not make real suggestion improvement...\n",
       "4      4              sir hero chance remember page that be"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('pre_data/df_train_spacycorr.csv', sep=';')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
