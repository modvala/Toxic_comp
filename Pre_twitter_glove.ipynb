{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import textacy\n",
    "#import en_core_web_md\n",
    "import sematch\n",
    "import gensim\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "    \n",
    "from clean_utils import *\n",
    "from parser_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from twit_prepocess import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "prefix_re = spacy.util.compile_prefix_regex(tuple(i for i in nlp.Defaults.prefixes if i not in '<'))\n",
    "suffix_re = spacy.util.compile_suffix_regex(tuple(i for i in nlp.Defaults.suffixes if i not in '>'))\n",
    "infix_re = spacy.util.compile_infix_regex(tuple(i for i in nlp.Defaults.infixes if i not in '<'))\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lemmat_twitter(df, shapes):\n",
    "    t = time.time()\n",
    "    print('clean and twitter tocken', df.shape)\n",
    "    df = df.apply(lambda x: clean(x))\n",
    "    df = df.apply(lambda x :tokenize(x))\n",
    "    df_train_tw = df.iloc[:shapes[0]]\n",
    "    df_test_tw = df.iloc[shapes[0]:]\n",
    "    print('Lemmatizing text.', df.shape)\n",
    "    q1 = []\n",
    "    for doc in tqdm(nlp.pipe(df, n_threads=8, batch_size=10000)):\n",
    "        word_list = []\n",
    "        for c in doc:\n",
    "            if c.lemma_ !='-PRON-':\n",
    "                word_list.append(c.lemma_)\n",
    "            else:\n",
    "                word_list.append(c.text)\n",
    "        q1.append(' '.join(i for i in word_list))\n",
    "    q1 = pd.DataFrame(q1, columns=['comment_text'])\n",
    "    print('Cleaning based on forums functions.', df.shape)\n",
    "    q1.comment_text = q1.comment_text.apply(lambda x: (' '.join([correction(i) for i in x.split(' ')])))\n",
    "    df_train_correct = q1.iloc[:shapes[0]]\n",
    "    df_test_correct = q1.iloc[shapes[0]:]\n",
    "    print('Text cleaning done, time it took:', time.time() - t, q1.shape)\n",
    "    return df_train_correct, df_test_correct, df_train_tw , df_test_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NROWS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv('data/train.csv', sep=',', index_col='id', nrows=NROWS)\n",
    "shapes = data.shape\n",
    "data['train'] = 1\n",
    "data = pd.concat([data, pd.read_csv('data/test.csv', sep=',', index_col='id', nrows=NROWS)], axis=0)\n",
    "data.train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec('vocs/glove.twitter.27B.200d.txt', 'vocs/glove.twitter.27B.200d_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('vocs/glove.twitter.27B.200d_w2v.txt')\n",
    "\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS[\"am\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocs/words_glove_twitter_for_check_spell.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(WORDS, 'vocs/words_glove_twitter_for_check_spell.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = joblib.load('vocs/words_glove_twitter_for_check_spell.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean and twitter tocken (312735,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing text. (312735,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312735it [1:09:17, 75.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning based on forums functions. (312735,)\n",
      "Text cleaning done, time it took: 4345.021574258804 (312735, 1)\n"
     ]
    }
   ],
   "source": [
    "df_train_corr, df_test_corr, df_train_tw, df_test_tw = clean_lemmat_twitter(data.comment_text, shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 1), (153164, 1), (159571,), (153164,))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_corr.shape, df_test_corr.shape, df_train_tw.shape, df_test_tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "More\n",
      "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
      "\n",
      "There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
      "\" more i cannot make any real suggestion on improvement - i wonder if the section statistic should be later on , or a subsection of \" \" type of accident \" \" - i think the reference may need tidy so that they be all in the exact same format ie date format etc . i can do that later on , if no - one else do first - if you have any preference for format style on reference or want to do it yourself please let me know . there appear to be a backlog on article for review so i guess there may be a delay until a reviewer turn up . it be list in the relevant form eg wikipedia : good_article_nomination <hashtag>   transport \"\n",
      "\"\n",
      "more\n",
      "i can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
      "\n",
      "there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations<hashtag>  transport  \"\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(data.comment_text[i])\n",
    "print(df_train_corr.comment_text[i])\n",
    "print(tokenize(data.comment_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_corr.to_csv('pre_data/df_train_tw_corr.csv', index = True, sep=';', index_label='index')\n",
    "df_test_corr.to_csv('pre_data/df_test_tw_corr.csv', index = True, sep=';', index_label='index')\n",
    "df_train_tw.to_csv('pre_data/df_train_tw.csv', index = True, sep=';', index_label='index')\n",
    "df_test_tw.to_csv('pre_data/df_test_tw.csv', index = True, sep=';', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanationwhy edit make username hardcore met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d'aww match background colour i be seemingly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man i be really try edit war -PRON- be guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>morei can not make real suggestion improvement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir hero chance remember page that be</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                       comment_text\n",
       "0      0  explanationwhy edit make username hardcore met...\n",
       "1      1  d'aww match background colour i be seemingly s...\n",
       "2      2  hey man i be really try edit war -PRON- be guy...\n",
       "3      3  morei can not make real suggestion improvement...\n",
       "4      4              sir hero chance remember page that be"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('pre_data/df_train_spacycorr.csv', sep=';')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
